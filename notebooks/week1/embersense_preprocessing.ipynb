{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EmberSense Week 1 Preprocessing Notebook\n",
        "\n",
        "This notebook annotates, cleans, and prepares the EmberSense multi-modal dataset. Run end-to-end via `scripts/preprocessing/run_preprocessing.py` for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import Audio, display\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ROOT = Path.cwd().parents[1]\n",
        "RAW_AUDIO = PROJECT_ROOT / \"data\" / \"raw\" / \"audio\"\n",
        "RAW_SENSORS = PROJECT_ROOT / \"data\" / \"raw\" / \"sensors\"\n",
        "LABELS_DIR = PROJECT_ROOT / \"metadata\" / \"labels\"\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_labels = pd.read_csv(LABELS_DIR / \"audio_event_annotations.csv\")\n",
        "sensor_segments = pd.read_csv(LABELS_DIR / \"sensor_segment_annotations.csv\")\n",
        "\n",
        "print(f\"Loaded {len(audio_labels)} audio events, {len(sensor_segments)} sensor segments\")\n",
        "audio_labels.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_SR = 16_000\n",
        "TARGET_DURATION = 10.0\n",
        "\n",
        "\n",
        "def load_audio(row):\n",
        "    path = PROJECT_ROOT / row.file_path\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "    audio, sr = librosa.load(path, sr=TARGET_SR, mono=True)\n",
        "    return audio\n",
        "\n",
        "\n",
        "def normalize_duration(audio: np.ndarray, sr: int = TARGET_SR) -> np.ndarray:\n",
        "    target_len = int(TARGET_DURATION * sr)\n",
        "    if len(audio) > target_len:\n",
        "        return audio[:target_len]\n",
        "    if len(audio) < target_len:\n",
        "        reps = int(np.ceil(target_len / len(audio)))\n",
        "        tiled = np.tile(audio, reps)\n",
        "        return tiled[:target_len]\n",
        "    return audio\n",
        "\n",
        "\n",
        "def lufs_normalize(audio: np.ndarray) -> np.ndarray:\n",
        "    peak = np.max(np.abs(audio)) + 1e-9\n",
        "    return 0.8 * (audio / peak)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_records = []\n",
        "for _, row in audio_labels.iterrows():\n",
        "    try:\n",
        "        audio = load_audio(row)\n",
        "    except FileNotFoundError as exc:\n",
        "        print(f\"Missing audio: {exc}\")\n",
        "        continue\n",
        "    audio = normalize_duration(audio)\n",
        "    audio = lufs_normalize(audio)\n",
        "    out_path = PROCESSED_DIR / \"audio\" / f\"{row.event_id}.npy\"\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    np.save(out_path, audio)\n",
        "    processed_records.append({\"event_id\": row.event_id, \"output\": str(out_path)})\n",
        "\n",
        "pd.DataFrame(processed_records).head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_sensor_parquet(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_parquet(path)\n",
        "\n",
        "\n",
        "sensor_outputs = []\n",
        "for _, row in sensor_segments.iterrows():\n",
        "    path = PROJECT_ROOT / row.file_path\n",
        "    if not path.exists():\n",
        "        print(f\"Missing sensor file: {path}\")\n",
        "        continue\n",
        "    df = load_sensor_parquet(path)\n",
        "    window = df.loc[row.start_iso:row.end_iso]\n",
        "    window = window.resample(\"1S\").interpolate(limit=60)\n",
        "    norm = (window - window.mean()) / window.std().replace(0, 1)\n",
        "    out_path = PROCESSED_DIR / \"sensors\" / f\"{row.segment_id}.parquet\"\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    norm.to_parquet(out_path)\n",
        "    sensor_outputs.append({\"segment_id\": row.segment_id, \"output\": str(out_path)})\n",
        "\n",
        "pd.DataFrame(sensor_outputs).head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "split_manifest = {\n",
        "    \"version\": \"0.1.0\",\n",
        "    \"seed\": 42,\n",
        "    \"splits\": {}\n",
        "}\n",
        "\n",
        "train, test = train_test_split(audio_labels[\"event_id\"], test_size=0.2, random_state=42)\n",
        "train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
        "split_manifest[\"splits\"][\"train\"] = train.tolist()\n",
        "split_manifest[\"splits\"][\"val\"] = val.tolist()\n",
        "split_manifest[\"splits\"][\"test\"] = test.tolist()\n",
        "\n",
        "split_path = PROJECT_ROOT / \"metadata\" / \"splits\" / \"dataset_splits.json\"\n",
        "split_path.write_text(json.dumps(split_manifest, indent=2))\n",
        "split_manifest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = {\n",
        "    \"generated_at\": pd.Timestamp.utcnow().isoformat(),\n",
        "    \"audio_events\": {\n",
        "        \"total_records\": int(audio_labels.shape[0]),\n",
        "        \"class_counts\": audio_labels[\"label\"].value_counts().to_dict(),\n",
        "        \"avg_duration_s\": float((audio_labels[\"end_s\"] - audio_labels[\"start_s\"]).mean()),\n",
        "    },\n",
        "    \"sensor_segments\": {\n",
        "        \"total_records\": int(sensor_segments.shape[0]),\n",
        "        \"class_counts\": sensor_segments[\"label\"].value_counts().to_dict(),\n",
        "        \"avg_window_minutes\": float(\n",
        "            (pd.to_datetime(sensor_segments[\"end_iso\"]) - pd.to_datetime(sensor_segments[\"start_iso\"])).dt.total_seconds().mean() / 60.0\n",
        "        ),\n",
        "    },\n",
        "}\n",
        "summary_path = PROJECT_ROOT / \"artifacts\" / \"week1\" / \"dataset_summary.json\"\n",
        "summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "summary_path.write_text(json.dumps(summary, indent=2))\n",
        "summary\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
