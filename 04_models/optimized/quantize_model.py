#!/usr/bin/env python3
"""
Quantize EmberSense models for MCU deployment using Edge Impulse.

Usage:
    python 04_models/optimized/quantize_model.py \
        --input-model 04_models/advanced/versions/v2.0/model.h5 \
        --output-dir 04_models/optimized/quantized/v2.0_int8 \
        --method int8 \
        --edge-impulse-project-id <PROJECT_ID>
"""

from __future__ import annotations

import argparse
import json
import subprocess
import time
from pathlib import Path
from typing import Dict, Any


def load_quantization_config(config_path: Path) -> Dict[str, Any]:
    """Load quantization configuration."""
    return json.loads(config_path.read_text(encoding="utf-8"))


def quantize_via_edge_impulse(
    input_model: Path,
    output_dir: Path,
    method: str,
    project_id: str,
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """Quantize model using Edge Impulse CLI."""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"[quantization] Starting {method} quantization via Edge Impulse")
    
    # Edge Impulse quantization command
    ei_settings = config.get("edge_impulse_settings", {})
    
    log_path = output_dir / "quantization_log.txt"
    with log_path.open("w", encoding="utf-8") as f:
        f.write(f"Quantization started: {time.strftime('%Y-%m-%dT%H:%M:%SZ')}\n")
        f.write(f"Input model: {input_model}\n")
        f.write(f"Method: {method}\n")
        f.write(f"Edge Impulse Project ID: {project_id}\n\n")
        
        f.write("Command (requires EI API key):\n")
        f.write(f"edge-impulse-cli --api-key $EI_API_KEY --project-id {project_id} \\\n")
        f.write("  optimize model \\\n")
        f.write(f"  --quantization {ei_settings.get('quantization', 'int8')} \\\n")
        f.write(f"  --optimization {ei_settings.get('optimization', 'latency')} \\\n")
        f.write(f"  --target-device {ei_settings.get('target_device', 'cortex-m4f')} \\\n")
        f.write(f"  --output {output_dir / 'model.eim'}\n")
    
    # Create metadata
    metadata = {
        "quantization_method": method,
        "input_model": str(input_model),
        "output_path": str(output_dir / "model.eim"),
        "edge_impulse_settings": ei_settings,
        "quantized_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "status": "pending_api_key",
    }
    
    metadata_path = output_dir / "quantization_metadata.json"
    metadata_path.write_text(json.dumps(metadata, indent=2), encoding="utf-8")
    
    print(f"[quantization] Configuration saved to {metadata_path}")
    print(f"[quantization] NOTE: Actual quantization requires Edge Impulse API key")
    
    return metadata


def quantize_via_tensorflow_lite(
    input_model: Path,
    output_dir: Path,
    method: str,
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """Quantize model using TensorFlow Lite."""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"[quantization] Starting {method} quantization via TensorFlow Lite")
    
    # Create TFLite quantization script
    script_path = output_dir / "quantize_tflite.py"
    script_content = f'''#!/usr/bin/env python3
"""
TensorFlow Lite quantization script.
Generated by quantize_model.py
"""

import tensorflow as tf
import numpy as np

# Load model
model = tf.keras.models.load_model("{input_model}")

# Representative dataset generator
def representative_dataset_gen():
    # Load validation samples for calibration
    # This is a placeholder - implement based on your dataset
    for _ in range(100):
        yield [np.random.randn(1, 625, 13).astype(np.float32)]

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)

if "{method}" == "int8":
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_dataset_gen
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.int8
    converter.inference_output_type = tf.int8

tflite_model = converter.convert()

# Save quantized model
output_path = "{output_dir / 'model.tflite'}"
with open(output_path, 'wb') as f:
    f.write(tflite_model)

print(f"Quantized model saved to {{output_path}}")
print(f"Model size: {{len(tflite_model) / 1024:.2f}} KB")
'''
    
    script_path.write_text(script_content, encoding="utf-8")
    
    metadata = {
        "quantization_method": method,
        "input_model": str(input_model),
        "output_path": str(output_dir / "model.tflite"),
        "quantized_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "status": "script_generated",
    }
    
    metadata_path = output_dir / "quantization_metadata.json"
    metadata_path.write_text(json.dumps(metadata, indent=2), encoding="utf-8")
    
    print(f"[quantization] TFLite script generated: {script_path}")
    return metadata


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Quantize models for MCU deployment.")
    parser.add_argument(
        "--input-model",
        type=Path,
        required=True,
        help="Path to input model (H5 or EIM)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        required=True,
        help="Output directory for quantized model",
    )
    parser.add_argument(
        "--method",
        type=str,
        choices=["int8", "int8_aware", "dynamic"],
        default="int8",
        help="Quantization method",
    )
    parser.add_argument(
        "--tool",
        type=str,
        choices=["edge_impulse", "tensorflow_lite"],
        default="edge_impulse",
        help="Quantization tool",
    )
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("04_models/optimized/configs/quantization_config.json"),
        help="Quantization configuration file",
    )
    parser.add_argument(
        "--edge-impulse-project-id",
        type=str,
        help="Edge Impulse project ID (required for EI tool)",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    config = load_quantization_config(args.config)
    
    if args.tool == "edge_impulse":
        if not args.edge_impulse_project_id:
            raise ValueError("--edge-impulse-project-id required for Edge Impulse quantization")
        metadata = quantize_via_edge_impulse(
            args.input_model,
            args.output_dir,
            args.method,
            args.edge_impulse_project_id,
            config
        )
    elif args.tool == "tensorflow_lite":
        metadata = quantize_via_tensorflow_lite(
            args.input_model,
            args.output_dir,
            args.method,
            config
        )
    else:
        raise NotImplementedError(f"Tool {args.tool} not implemented")
    
    print(f"[quantization] Quantization metadata saved")
    print(f"[quantization] Output directory: {args.output_dir}")


if __name__ == "__main__":
    main()

